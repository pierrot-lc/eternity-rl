@document.meta
title: todo
authors: pierrotlc
categories: notes
created: 2023-03-08
updated: 2024-02-29T23:39:33+0100
@end

* TODO
  - Verify enhancements:
  -- ( ) ReLU entropy
  -- ( ) Action-specific weighted entropy
  -- ( ) Separated actor/critic network
  -- ( ) CNN backbone encoder
  - ( ) Checker https://arxiv.org/abs/2110.00641
  - ( ) Regarder s'il est possible d'optimiser l'env batché
  - ( ) Restart of the value network training from time to time.
  - ( ) Use random envs. Start from optimal env, scramble with random
        actions, with a random number of actions. Do scramble the original
        env as well. This is akin to local search with some noise added
        from time to time.

* MCTS
  - ( ) Ajouter des métriques pour mieux visualiser le parcours des arbres. Avoir
        un moyen de voir la quantité d'exploration et d'exploitation.
  -- ( ) Tree depth
  -- ( ) Visit count distribution
  -- ( ) Average node value vs average node ucb score
  -- ( ) Same-node count

  - (x) Regarder pour améliorer le MCTS avec rewards intermédiaires:
  -- (x) Le score d'un noeud devrait être la reward pour arriver à ce noeud + valeur du critique.
  -- (x) Lors de la backprop, il faut prendre la valeur du noeud enfant + la somme des rewards de la trajectoire.
  -- (x) Ajouter aussi le produit gamma.
  -- (x) Voir https://github.com/tmoer/alphazero_singleplayer.

  - Ajouter la normalisation MinMax des scores pour ne pas avoir à tuner la constante d'exploration.
    Voir https://arxiv.org/pdf/1911.08265.pdf page 12.

* Why it does not work?
  - Is that possible that the model struggles at some point because the value function
    has overfit the model capacity?
  - (x) Is that possible that the values are not good at some point because the model
    is rewarded at the end of the rollout for the value of the final state.
  - (x) This means that at some point the model is able to reach a very good state
    but the value function does not recognize this and is learning that good moves
    values bad. THIS LOOKS GOOD!
  - Maybe use a composite reward ? Deltas rewards could help for credit assignment.
  - The issue is that we cannot give the value of the best previous state to
    the value network will struggle to not overfit onto it and collapse. And we should
    help the value network in some way? (should we?)
  - Maybe do smaller rollouts and do not reset all envs between two rollouts.
    This would allow for better credit assignment.

* Enhancements
  - (x) Rotation and symmetry invariant.
  - (_) 2D RoPE.

* Questions
  - Is the model better with conv layers, transfo layers, or little bit of both?
  -- Should I use positional encoding or are the conv layers enough?
  -- Ended up using transformer layers only, to have a simpler architecture and to
     make sure the model can easily attend to long interactions.
  - Does the perf scale with model size really?
  - Is curriculum learning helpful?
  - Should I let the model train longer?
  - Does random instances help?
  - Is it important to randomly reset the instances?
  - How to escape from the final plateau?
  - How to force the model to output different actions.
  -- Search strategies.
  - Episodic vs never-ending episodes.
  - CNN vs Transformer backbone.

* Ideas
  - Do MCTS once the model is trained and see if it is better than the model alone.
  - Use MC sampling to sample the actions based on the result of the multiple simulations.
  - Once a model is trained, it is possible to do a soft MCTS:
  ~~ Start from a random state.
  ~~ Duplicate the state into many (many) batch games.
  ~~ Do a rollout with random sampling for each games in the batch.
  ~~ Aggregate the final results and apply the actions that led to the best final (maximum) state.
  ~~ Go back to point B
  -- This should enhance the overall search for a solution. It may be possible to apply this search
     with many different games in parallel, and to make the search over and over.
  - Could it be possible to coordinate the search among the batch? A kind of multi-agent
    search. Avoid states that are being searched by others.
  -- ex: VAE giving a sense of how close two game states are. By the end of a rollout
     I can reward agents that have looked away from other agents, and that have not
     stayed in the same states during its rollout.
  - Better weight the entropy action loss.
  -- The first action can have great entropy, the last two should not!
  - Use a rotation and symmetry invariant model.
  - Use 2D RoPE.
  - Do not share parameters between actor and critic models.
  - Asymmetric self-play
  -- An agent is trained to challenge another agent to solve the puzzle.
  -- The first agent starts from a random solved puzzle, and mixes the pieces however it wants.
  -- The second agent tries to solve it back.
  -- The second agent is also trained from time to time on the real target puzzle.
  -- The games should gradually increase in difficulty by doing so.
  -- See {https://arxiv.org/abs/1703.05407}.
  - Add tricks from {https://iclr-blog-track.github.io/2022/03/25/ppo-implementation-details/}.
  - Construct artificial envs (so that we know the target state) and use this paper to
    learn a policy that solves it: {https://arxiv.org/abs/2310.06794v1}.
  - Ideally the model needs an architectural bias that allows him
    to play multiple rounds in its latent space.
