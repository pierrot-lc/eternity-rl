@document.meta
title: random
description: Some random thoughts
authors: pierrotlc
categories: Notes
created: 2023-06-04
updated: 2023-11-11T23:07:14+0100
version: 1.1.1
@end


* Computing the logprobs with logits and softmax
  It is not the same to compute the logprobs from the outputted logits and
  from the softmax distribution.

  @code python3
  distrib = torch.distributions.Categorical(logits=logits)
  logprobs = distrib.log_prob(actions)
  @end

  @code python3
  distrib = torch.distributions.Categorical(probs=torch.softmax(logits))
  logprobs = distrib.log_prob(actions)
  @end

  The second one is the one that should be used. I think that the first
  one is either wrong or inefficient.

* Instability in the training
  The reward normalization can lead to instabilities in the training. It
  is important to take this into account and to use a rather large
  epsilon.

  A recurrent policy is also hard to train since it can lead to huge BPTT
  gradient updates. To counter this, I regularly stop the gradient during
  the rollout. Would it be feasible to stop the gradient at each step?
  This would make the use of a replay buffer easier.

* Actor/critic
  My implementation seems to not be working so well, or at most to do as
  the base implementation. It may be because the current reward estimation
  is already good enough thanks to the high batch MC sampling.

  It also introduces a lot of hyperparameters to tune, so I decided to
  remove it.

  Finally, I have to use it back since i'm training in a never-ending
  environment. To have access to a return estimation, I have to estimate
  the values using a value function.

  It is advised to have two separate networks for the actor and the
  critic. After little testing, I have not found so much differences. But
  I have found a $30%$ increase in total time per epoch. This is because
  during my training a lot of the compute is spent on the model since the
  rest is pretty well optimized and batched. Note that I did not really
  spend a lot of time experimenting this. It may be worth to have two
  small models that are faster than a single big shared model for both
  actor and critic.

  Finally, I have separated my models since it allows me to train the
  critic more independently. Doing so, I can train the critic with samples
  from another trajectories (MCTS sampling for example).

  *It is important that the critic does not overfit the actor actions. At
  some point, the critic gets use to the mean actor performance. When that
  happen, the critic is not guiding the actor anymore. The only way for
  the actor to improve is to randomly find good moves that the critic did
  not predict.* This is why you should have a low learning rate for the
  critic.

  The issue is:
  - Either the actor just cannot randomly find the good moves. The
    probability is too low (need long-term planning).
  - Either the critic has so much penalised the actor on the potential
    good moves that the actor just won't do the unknown good moves.

* Reward modeling
  The `win` reward is good because it does not optimize for the number of
  steps to win, nor for intermediate easy rewards. But it has a problem:
  long rollouts tend to all collapse to the mean state. The mode is not
  rewarded for the maximum intermediate state.

  The new reward called `max-cut` provides a more stable training, and is
  way more sample efficient.

  *The reward should be designed so that the model is not locked in
  a local minimum of the puzzle.* When locked into a local minimum, the
  model tends to just to the same actions in a loop.

  *The reward should take into account that after a good reached state by
  the model the following actions will degrade and eventually regress to
  the mean the value of the state.* This means that actions should be
  rewarded by the best value reached during rollout (at least for the
  following encountered states).

  - $current_matches - previous_matches$: model will be easily locked into
    a local minimum.
  - $max(matches - previous_max_matches, 0)$: this is a good reward, as it
    promotes only new best solutions and does not penalize the model for
    degrading a solution.
  - $max_matches$: the ending value is simply the max matches found. Could
    also be applied at each step.

  Do I need to help the critic in some way when the information about the
  max matches is used for the reward computation ? It worked without
  helping the critic that much but it maybe could help or lead to a mode
  collapse of the critic by overfitting to easy targets.

* Replay buffer
  The replay buffer is primarily used to be more sample efficient by
  reusing past samples during the training. But in my case, collecting the
  rollout samples is pretty fast so that I can do without such a buffer.

  Being able to train without a replay buffer is nice because it makes it
  easier to implement tricky policies such as RNNs (useful for planning).
  On the other hand, a replay buffer can be useful to implement tricky
  sampling policies such as the prioritized experience replay.

* Rotation equivariant
  The game is strictly rotation equivariant, meaning that the model should
  output the same rotated actions to a rotated board. This can be
  integrated in the algorithm in multiple ways: ~ Properly randomize the
  board state, so that the model sees a lot of diverse board states and
  learns to be equivariant by itself. ~ Add a regularization loss that
  directly enforce the model to be equivariant to rotations of the board.
  ~ Design the model so that it is naturally equivariant to rotations.

  Note that the game is also strictly equivariant to horizontal and
  vertical flips.

  The best thing to do is to design the model to be equivariant to
  rotations and flips. To do so, the encoder can use
  a {https://arxiv.org/abs/1602.07576}[group equivariant CNN] and/or
  a simple transformer encoder (with a symmetrical equivariant positional
  encoding, if used). The decoder can use a pointer network to select the
  tile to swap, and since the rolls are relative to the current tile
  placement, it can be done with a simple linear layer.

* Parallel exploration
  Using pure MCTS is not cool because it is hard to implement in parallel.
  Another way of exploring potential futures and to plan is to duplicate
  each instance and to do batched steps forward. It can be used to
  estimate what the next best action is. This process can also be repeated
  multiple times to trade time for memory.

  A kind of beam search.

* Potentially never-ending episodes
  A proper way to train the model is to use never-ending episodes. Doing
  so, the return is impossible to truly compute with MCTS sampling, but it
  can be estimated with a value network.

  But it is hard to train a value network in my environment since the
  model can kind of go back in time. By switching pieces, it is easy to go
  to a previously encountered state. The value network has to take into
  account this fact, and the training becomes harder.

  The reward is also very important. A good reward shaping will make it
  easier for the model to quickly understand what the goal is.

  Hypothesis: this training is very prone to local minimas. Once the board
  is almost complete, the model is having a really hard time to finish the
  puzzle because most of the time it means that many pieces have to be
  swapped around. Doing so is asking for a lot of exploration, arguably
  too much exploration.

  To avoid local minimas, the model should build the board from scratch
  everytime and only be given the reward at the end of the episode (like
  I used to to at the beginning).

  It does seem like this hypothesis is not true in the end. The model sees
  the result of the multiple actions. At least, for my current training
  setup, havin a never-ending or fixed horizon rollout does not change the
  local minimas the model is prone to.

* About on-policy learning and exploration
  It's harder to implement a forced exploration or exploitation when using
  an on-policy algorithm. You can't use a tempered softmax, epsilon-greedy
  or a mix of samplings. That's a shame!

  It could be interesting to sample the tile selection with a tempered
  softmax and the tile rotation with epsilon-greedy.

* Planning with on-policy methods
  Turns out it is very hard (if not impossible) to train a policy with
  policy gradient methods to solve planning problems. Solving a puzzle is
  a planning problem, and as such I think you should incorporate
  a planning strategy in your training. But you can't just use MCTS to
  collect your rollouts, as it becomes an off-policy rollout!

  But it also turns out it is not really possible to train an off-policy
  on the eterniy puzzle, because it has too many actions! When the action
  is a simple swap, this leads to more than 1000s possible actions (the
  number of action grows polynomially). This means that to be able to use
  planning training such as AlphaZero, you need to find a new set of
  actions that is much lighter.

  So what to do? An idea would be to train an on-policy actor/critic
  model, but with a critic that is trained based on MCTS rollouts. You can
  keep training the policy as usual, based on the values predicted by the
  critic, in an on-policy manner. But you train the critic only on the
  MCTS rollouts, using the policy to explore the trees. Since the number
  of actions is huge, you can sample the most probable actions and expand
  the nodes using those actions.

* Resetting an environment during rollout collection
  I met some instabilities when training on small puzzles, and I couldn't
  find where they could come from. The following blog post
  {https://iclr-blog-track.github.io/2022/03/25/ppo-implementation-details/}[37 implementation details of PPO] gave me the answer.

  When collecting experience replay samples, you /should/ reset the
  environment once an environment is either done or truncated. Doing this
  ensure that your replay buffer will always have the same fixed size. My
  instability problem was due to the fact that once the model was acing
  the small puzzle it was looking at, it started to be so good that each
  environment in the batch was done very quickly. This lead to a replay
  buffer that is smaller than what was expected and my model would be
  trained on the same samples many times before collecting new samples
  (because I used a fixed number of iterations over the buffer with random
  sampling with replacement).

  All and all, not resetting the environment lead to an unstable training
  due to the fact that my on-policy algorithm was not really on-policy
  anymore, since samples were used many times during training.

  Note that to implement this you need to properly take into account the
  done and truncated flags of the rollout when computing advantages and
  value targets. Moreover, doing this efficiently on GPU is hard.

* RL bugs are hard
  Many months later, I found out that when I reset some specific envs of
  the batch, the others are also affected. This means that my training was
  wrong all along (estimation of the advantage, value targets, and overall
  environment length). This problem was undetected thanks to an `assert`
  that was missing in a unit-test.

  Make sure the value agent does not know what actions are sampled. Since
  my actor needs to know which sub-action is sampled before outputting the
  rest of the sub-action distributions, the value network can also see
  them. In this case, training was collapsing at some point.

  Always simulate results of the rollouts for the rewards and returns
  computations. Avoid easy local minimas. Avoid regressions to the mean.

* Transformer enhancements
  When reading recent litterature about LLM, I saw that everyone is using
  a slightly different version of the classical Transformer architecture.

  Improvements:
  - Pre-norm.
  - SwiGLU activation function (https://arxiv.org/abs/2002.05202).
  - RMSNorm (https://arxiv.org/abs/1910.07467).
  - Rotary positional encoding (https://arxiv.org/abs/2104.09864).

  They are either here to stabilize or speed up the training. I did not
  implemented RMSNorm since it is only here to replace the LayerNorm which
  is not arguably what slows me the most (my hidden dimensions are small).

  I saw those enhancements in the {https://arxiv.org/abs/2302.13971}[LLaMa paper].

* Dynamic MCTS
  The problem I have with the original MCTS is that it needs to expand all
  children when expanding a new node. Since I have a terrible amount of
  actions in my action space, I can't afford it.

  The way to counter this is to use the policy to sample actions that will
  be used to create new nodes. The question becomes:

  How do I decide how many new nodes do I sample each time I expand a node
  ?

  We could think about a sampling strategy that makes sure we sample the
  n-most probable candidates. Others ideas could be taken from the LLM
  sampling litterature (p-sampling).

  Another idea would be to always sample one action (randomly) and to have
  a mecanism to sample to childs for each node of the tree (not only at
  the leafs!). For example, we could have a fictious child node which,
  when selected, means that the father should sample a new child because
  others are too explored.

* A batched MCTS
  I have implemented a simplified MCTS that can search in a batch of
  environments. This means that I can collect rollouts in parallel, but it
  doesn't search in a single tree in parallel.

  To do so, I have made a simplification: the number of child each node
  can have is bounded. I have made such simplification so that I can bound
  the total number of nodes in the tree.

  Here's the main ideas:
  - I can compute the number of nodes in a single tree with the following
    equation: $total_nodes = n_simulations * n_childs_per_nodes + 1$ At
    each simulation, we will expand a node and create at most
    $n_childs_per_nodes$. The $+ 1$ is to account for the root node.
  - Each node is given an id (starting at $0$ for the root node).
  - An array of shape `[batch_size, total_nodes, n_childs_per_nodes]` is
    used to know the ids of the childs of a given node id in each MCTS
    trees.
  - An array of shape `[batch_size, total_nodes]` is used to know the ids
    of the parent of any node id in each MCTS trees.
  - The same general idea is used to know the number of visits, the total
    score and other properties of each node in each trees.
  - Each step of the MCTS algorithm is batched along all the trees. For
    example, when selecting the next node to expand, I dive in all trees
    at the same time until all trees have reach a leaf.
  - When expanding a batch of nodes, I sample the new childs according to
    the policy. I do not make sure that childs are differents. Also, I use
    the critic network to immediately compute the value of the new nodes.
  - Of course, since everything is done with tensor operations, those
    operations are done on GPU and can be done with the standard multi-GPU
    distributed training.
  - Since the nodes are not fully expanded, I have to allow terminated
    nodes to be visited again. This is because otherwise the visit count
    will not reflect the value of those nodes.

  Because the maximum number of nodes is known, I can stack all trees
  together inside tensors.

  Needless to say, a beam search would have been much easier to implement,
  but I wanted to try this out.

** What I learns from the MCTS implementation

   It turns out it is not easy to implement a MCTS properly, especially
   with my ideas of doing a partial batched MCTS. The fact that not all
   nodes are sampled and that my env is fully reversible makes the tree
   search really difficult.

   First, I ended up convinced that it is not possible to learn a policy
   if the tree is only partially expanded. This is because the policy is
   used for both sampling the new nodes and learning which node to sample.
   It collapse easily and learn to sample the same node over and over
   again and so that it is never wrong.

   Secondly, I think the fact that the env is fully reversible is a bi
   problem. It makes the tree search really hard there is no bad state to
   reach that is not recoverable later on. This tends to make the nodes
   all have the same average estimated value. I have tried to use a very
   low `gamma` and to maximize the exploitation but whatever my values was
   the search looked like it was not able to find the really good actions.

   Finally, I'm not even sure that my implementation is correct. It is
   really hard to find good resources on the subject. I have shared what
   I have found online in the code. Moreover in my case I have only
   a partially expanded MCTS so maybe this whole thing can't work without
   some heavy modifications.

* On the difficulty of extrem exploration

  Solving the puzzle necessarily requires deep exploration. Even for the
  simple 4x4 puzzle, the model tends to be stuck in a local minimum. This
  is why I use some very aggressive entropy regularization. Along with
  this, I use a very low learning rate for the critic. This is because the
  critic tends to overfit the actor and to guide it to the mean
  performance.

  This is a very hard problem to solve. The model needs to exploit the
  good actions is has found while still being able to explore. Even the
  MCTS does not work here, as the environment is fully reversible (at
  least that's my understanding of it).

  Here are some untested ideas that could help:
  - Reset the critic weights from time to time so that it can start over.
  - Add some noises in the critic and policy weights during training.
  - Add penalization to commonly predicted actions in the rollouts.
  - Use randomly generated problems.
